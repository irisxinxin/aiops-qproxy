{"title": "2.0 三", "keys": ["cat:elasticsearch", "cat:error", "cat:network", "sev:*", "svc:omada-*", "svc:omada-central", "svc:vigi-*"], "priority": "HIGH", "prechecks": [], "actions": ["区升级\n期间使\n用Service updating… \nDuring this period, a \nsystem with user \naccounts of multiple \nservice areas cannot \ninstall new service.", "RD-Cloud&Back-End-Shared – Standard Emergency Measures For Omada(SRE Handover)\nOmada Common Error Codes  – 98NEED_CENTRAL_APPLI\nCATION_PERMISSION-84\n010The current Omada Central \nApplication permission is \nNone.", "Cannot operate site.central \n2.0操作\nsite权\n限校验The current Omada \nCentral Application \npermission is None.", "Cannot operate site.", "NEED_NETWORK_APPL\nICATION_PERMISSION-84\n011The current Omada Network \nApplication permission is \nNone.", "Cannot operate site.central \n2.0操作\nsite权\n限校验The current Omada \nNetwork Application \npermission is None."], "grafana": [], "notes": "", "refs": ["Standard Emergency Measures For Omada(SRE Handover)-v157-20250926_001057.pdf"]}
{"title": "RMISSION-84", "keys": ["cat:network", "sev:*", "svc:omada-*", "svc:omada-central", "svc:vigi-*"], "priority": "MIDDLE", "prechecks": [], "actions": ["013The current Omada Central \nApplication and Omada \nNetwork Application \npermission are None.", "Cannot \noperate site.central \n2.0操作\nsite权\n限校验The current Omada \nCentral Application \nand Omada Network \nApplication \npermission are None.", "Cannot operate site.", "NEED_CENTRAL_VMS_\nAPPLICATION_PERMIS"], "grafana": [], "notes": "", "refs": ["Standard Emergency Measures For Omada(SRE Handover)-v157-20250926_001057.pdf"]}
{"title": "SION-84", "keys": ["cat:network", "sev:*", "svc:omada-*", "svc:omada-central", "svc:vigi-*"], "priority": "MIDDLE", "prechecks": [], "actions": ["014The current Omada Central \nApplication and Omada \nGuard Application permission \nare None.", "Cannot operate \nsite.central \n2.0操作\nsite权\n限校验The current Omada \nCentral Application \nand Omada \nGuard Application \npermission are None.", "Cannot operate site.", "NEED_NETWORK_VMS_\nAPPLICATION_PERMIS"], "grafana": [], "notes": "", "refs": ["Standard Emergency Measures For Omada(SRE Handover)-v157-20250926_001057.pdf"]}
{"title": "SION-84", "keys": ["cat:elasticsearch", "cat:error", "cat:network", "sev:*", "svc:omada-*", "svc:omada-central", "svc:vigi-*"], "priority": "HIGH", "prechecks": [], "actions": ["015The current Omada Network \nApplication and Omada \nGuard Application permission \nare None.", "Cannot operate \nsite.central \n2.0操作\nsite权\n限校验The current Omada \nNetwork Application \nand Omada \nGuard Application \npermission are None.", "Cannot operate site.", "INVALID_TRANSFER_O\nWNER_ORG-84\n016Cannot transfer owner due to \naccount strategy upgrade.", "Please delete all \nSurveillance devices and add \nthem again before migration.不支持\n组织转\n移的\ncentral\n组织\n（旧\nvms 组\n织不支\n持）Cannot transfer \nowner due to account \nstrategy upgrade.", "Please delete \nall Surveillance \ndevices and add \nthem again before \nmigration."], "grafana": [], "notes": "", "refs": ["Standard Emergency Measures For Omada(SRE Handover)-v157-20250926_001057.pdf"]}
{"title": "1 https://pdconfluence.tp-link.com/pages/viewpage.action?pageId=43071067", "keys": ["sev:*", "svc:vigi-*"], "priority": "MIDDLE", "prechecks": [], "actions": ["– 3For the exception problem that has been handled, we summarize some experience.", "If new problems arise, expect to deal with them together and draw lessons from them.", "Log troubleshooting Guide -VMS1\nRD-Cloud&Back-End-Shared – Troubleshooting of VIGI - VMS Cloud"], "grafana": [], "notes": "", "refs": ["Troubleshooting of VIGI - VMS Cloud-v10-20250926_001218.pdf"]}
{"title": "1.1 SLA", "keys": ["sev:*", "svc:omada-controller", "svc:vigi-*"], "priority": "MIDDLE", "prechecks": [], "actions": ["Search for apis in the vms code controller class, and then search for logs using requestUri\nNam\neGrafana  VMS Cloud DashBoard3\nPass\nthro\nughCore - Device - RemoteControl\nLogi\nn\nGet \nPullS\ntrea\nm \nAddr\nessThere is a problem with the SLA/SLI of the vms.", "Because the reliability of the remote management device is insufficient, the SLA of the \npasthrough and related get device detail and playback search video   cannot meet the standards\nGrafana VMS SLA2\n\nRD-Cloud&Back-End-Shared – Troubleshooting of VIGI - VMS Cloud\nTroubleshooting Guide  – 5Get \nOrg \nList\nGet \nEven\nt \nPage"], "grafana": [], "notes": "", "refs": ["Troubleshooting of VIGI - VMS Cloud-v10-20250926_001218.pdf"]}
{"title": "1.2.2 vms-event-frontend", "keys": ["cat:cpu", "sev:*", "svc:vigi-*"], "priority": "HIGH", "prechecks": ["Temporary operation: \nincrease minReplicas\nexpand cpu resources\nOpen Grafana  VMS Event Frontend Dashboard5, Check \nEvent Frontend Call: TPS and avg costTime."], "actions": ["Grpc Server : TPS and avg costTime."], "grafana": [], "notes": "", "refs": ["Troubleshooting of VIGI - VMS Cloud-v10-20250926_001218.pdf"]}
{"title": "7 https://grafana.i.tplinknbu.com/d/anrQBsMHr/vms-ai-manager-dashboard?orgId=1&from=now-12h&to=now&timezone=browser&var-", "keys": ["cat:cpu", "sev:*", "svc:vigi-*"], "priority": "HIGH", "prechecks": [], "actions": ["datasource=f_ml_g-Vk&var-namespace=vms&refresh=5s\nTroubleshooting Guide  – 7•\n•\n•\na.", "b.", "•\na.", "b.", "•\na.", "b."], "grafana": [], "notes": "", "refs": ["Troubleshooting of VIGI - VMS Cloud-v10-20250926_001218.pdf"]}
{"title": "1.3 container memory usage is too high", "keys": ["cat:memory", "sev:*", "svc:vigi-*"], "priority": "MIDDLE", "prechecks": [], "actions": ["Alarm Name Alert Level Effective \nRegionExpression\nvms-core-server \ncontainer memory usage \nupper 0.95Tel prd-nbu-use1\nprd-nbu-euw1\nprd-nbu-aps1sum(container_memory_working_set_b\nytes{container!=\"POD\",container!=\"\", \ncontainer!=\"istio-proxy\", image!", "=\"\",pod=~\"vms-core-server.*\", \nnamespace=~\"vms\"}) by (pod, \ncontainer) / \nsum(kube_pod_container_resource_limi\nts{container!=\"POD\",pod=~\"vms-core-\nserver.*\", namespace=~\"vms\", \nresource=\"memory\"} > 0) by (pod, \ncontainer) > 0.95\nRD-Cloud&Back-End-Shared – Troubleshooting of VIGI - VMS Cloud"], "grafana": [], "notes": "", "refs": ["Troubleshooting of VIGI - VMS Cloud-v10-20250926_001218.pdf"]}
{"title": "8 https://pdconfluence.tp-link.com/display/cloudshare/VMS+AI+disconnection+Ops+Guidelines", "keys": ["cat:elasticsearch", "cat:memory", "sev:*", "svc:vigi-*"], "priority": "MIDDLE", "prechecks": [], "actions": ["Troubleshooting Guide  – 8•\n•Alarm Name Alert Level Effective \nRegionExpression\nvms-event-frontend \ncontainer memory usage \nupper 0.95Tel prd-nbu-use1\nprd-nbu-euw1\nprd-nbu-aps1sum(container_memory_working_set_b\nytes{container!=\"POD\",container!=\"\", \ncontainer!=\"istio-proxy\", image!", "=\"\",pod=~\"vms-event-frontend.*\", \nnamespace=~\"vms\"}) by (pod, \ncontainer) / \nsum(kube_pod_container_resource_limi\nts{container!=\"POD\",pod=~\"vms-event-\nfrontend.*\", namespace=~\"vms\", \nresource=\"memory\"} > 0) by (pod, \ncontainer) > 0.95\nvms-ai-manager \ncontainer memory usage \nupper 0.95Critical prd-nbu-use1\nprd-nbu-euw1\nprd-nbu-aps1sum(container_memory_working_set_b\nytes{container!=\"POD\",container!=\"\", \ncontainer!=\"istio-proxy\", image!", "=\"\",pod=~\"vms-ai-manager.*\", \nnamespace=~\"vms\"}) by (pod, \ncontainer) / \nsum(kube_pod_container_resource_limi\nts{container!=\"POD\",pod=~\"vms-ai-\nmanager.*\", namespace=~\"vms\", \nresource=\"memory\"} > 0) by (pod, \ncontainer) > 0.95"], "grafana": [], "notes": "", "refs": ["Troubleshooting of VIGI - VMS Cloud-v10-20250926_001218.pdf"]}
{"title": "1.3.3 other", "keys": ["cat:memory", "sev:*", "svc:vigi-*"], "priority": "MIDDLE", "prechecks": [], "actions": ["Temporary operation:\nredeploy\nincrease memory resource.", "RD-Cloud&Back-End-Shared – Troubleshooting of VIGI - VMS Cloud"], "grafana": [], "notes": "", "refs": ["Troubleshooting of VIGI - VMS Cloud-v10-20250926_001218.pdf"]}
{"title": "1.4 no traffic", "keys": ["cat:disk", "cat:network", "sev:*", "svc:vigi-*"], "priority": "MIDDLE", "prechecks": ["This can happen when the service is \nupdated\nCheck service status in rancher\nCheck use1 cluster Istio config, compare with code ,and fix it."], "actions": ["Alarm Name Alert Level Effective Region Expression\nvms event frontend no \ntrafficTel prd-nbu-use1\nprd-nbu-euw1\nprd-nbu-aps1sum(rate(event_frontend_requests_t\notal{namespace=\"vms\"}[2m])) == 0\nvms core server no \ntrafficTel prd-nbu-use1\nprd-nbu-euw1\nprd-nbu-aps1sum(rate(http_server_requests_sec\nonds_count{application=\"vms-core-\nserver\",namespace=~\"vms\", uri!", "~\".*actuator.*\"}[2m])) == 0\nNo traffic means the service is down or there is a routing problem .", "CDD/solution/devops_cicd9 ,branch \nvms_beta ,path k8s-values/${service}/prd/${region}/networking/istio.yaml \nGateway:  like vms-core-server-aps1-gw\nVirtualService:  like vms-event-frontend-aps1-vs\nDestinationRule ：like vms-event-frontend-aps1-dr\nRD-Cloud&Back-End-Shared – Troubleshooting of VIGI - VMS Cloud"], "grafana": [], "notes": "", "refs": ["Troubleshooting of VIGI - VMS Cloud-v10-20250926_001218.pdf"]}
{"title": "2.1.5 Mongodb success rate below 0.999", "keys": ["cat:error", "cat:network", "sev:*", "svc:vigi-*"], "priority": "HIGH", "prechecks": ["Alarm Name Alert Level Effective Region Expression\n【VMS】mongodb \noperation success rate \nbelow 0.999Tel\nprd-nbu-aps1\nprd-nbu-euw1\nprd-nbu-use1sum(rate(third_party_request_total{a\npplication=\"vms-core-\nserver\",errorCode=\"0\",target=\"mongo\"\n}[2m]))/\nsum(rate(third_party_request_total{a\npplication=\"vms-core-\nserver\",target=\"mongo\"}[2m])) < \n0.999\nCheck VMS Cloud DashBoard10 - MongoDB Call - MongoDB Call Success Rate\nAll methods report errors: There may be a problem with mongodb or network communication\nevent, message related methods reported an error: redeploy service."], "actions": ["(Historical experience: VMS \nmongo请求成功率降低11)\nSearch in Kibana12, If the log says \"duplicate key\",redeploy can fix it, otherwise it needs to be fed back to R&D \nfor analysis\n\nRD-Cloud&Back-End-Shared – Troubleshooting of VIGI - VMS Cloud"], "grafana": [], "notes": "", "refs": ["Troubleshooting of VIGI - VMS Cloud-v10-20250926_001218.pdf"]}
{"title": "100 < 99", "keys": ["cat:deploy", "cat:elasticsearch", "cat:error", "sev:*", "svc:vigi-*"], "priority": "HIGH", "prechecks": [], "actions": ["vms-core-server http error \ncode 5XXCriticalprd-nbu-aps1\nprd-nbu-euw1\nprd-nbu-use1increase(http_server_requests_second\ns_count{application=~\"vms-core-\nserver\", namespace=~\"vms\", \nstatus=~\"5[0-9]{2}\"} [5m]) > 0\nhistorical experience\nvms access S3 using the stroage component provided by smb, which manages connections to \ns3 through a thread pool.", "A previous problem with the getFileSize method causing the thread pool to fail to be released \nwas fixed in version 1.4.7 of this component.\nRD-Cloud&Back-End-Shared – Troubleshooting of VIGI - VMS Cloud"], "grafana": [], "notes": "", "refs": ["Troubleshooting of VIGI - VMS Cloud-v10-20250926_001218.pdf"]}
{"title": "15 https://kibana.tplinknbu.com/s/cdd_cloud_service/app/discover#/?_g=(filters:!(),refreshInterval:(pause:!t,value:0),ti", "keys": ["cat:error", "sev:*", "svc:vigi-*"], "priority": "HIGH", "prechecks": [], "actions": ["(from:now%2Fd,to:now%2Fd))&_a=(columns:!(exType,exMessage),filters:!((%27$state%27:(store:appState),meta:(alias:!n,disabled:!", "f,index:d4d45bd0-2813-11ed-ba5e-9372ec608ce2,key:statusCode,negate:!t,params:(query:%270%27),type:phrase),query:(match_phrase:\n(statusCode:%270%27)))),index:d4d45bd0-2813-11ed-ba5e-9372ec608ce2,interval:auto,query:(language:kuery,query:%27%27),sort:!())\nTroubleshooting Guide  – 13•\n•Alarm Name Alert Level Effective \nRegionExpression\nvms-core-server http error \ncode 4XX (excluding 429) \nexceed 10%Critical\nprd-nbu-aps1\nprd-nbu-euw1\nprd-nbu-use1100 * \nsum(increase(http_server_requests_se\nconds_count{application=~\"vms-core-\nserver\", namespace=~\"vms\", \nstatus=~\"4[0-9]{2}\"} [30m]) - \nincrease(http_server_requests_second\ns_count{application=~\"vms-core-\nserver\", namespace=~\"vms\", \nstatus=~\"429\"} [30m])) / \nsum(increase(http_server_requests_se\nconds_count{application=~\"vms-core-\nserver\", namespace=~\"vms\"} [30m])) > \n10\nOpen VMS Cloud DashBoard14 - API Gateway Call:\nSuccess rate: check api\nhttp 5XX: get url and exceptionType\nOpen Kibana15, get errorCode\n\nRD-Cloud&Back-End-Shared – Troubleshooting of VIGI - VMS Cloud"], "grafana": [], "notes": "", "refs": ["Troubleshooting of VIGI - VMS Cloud-v10-20250926_001218.pdf"]}
{"title": "20 https://grafana.tplinkcloud.com/d/fef0ba4oqxx4wc/vms-event-frontend-dashboard?", "keys": ["cat:cpu", "cat:disk", "cat:error", "cat:kafka", "cat:memory", "cat:network", "sev:*", "svc:vigi-*"], "priority": "HIGH", "prechecks": [], "actions": ["orgId=1&from=now-6h&to=now&timezone=browser&var-datasource=f_ml_g-Vk\nTroubleshooting Guide  – 161.Temporary operation: \nopen VMS Event Frontend Dashboard20 Event Frontend avg costTime , if istio ingressgateway duration >>\nEvent Frontend avg costTime and Event Frontend TPS is on the rise.", "May be server is unable to handle the device event pressure, increase vms-event-frontend pod number\n\nRD-Cloud&Back-End-Shared – Troubleshooting of VIGI - VMS Cloud\nTroubleshooting of VIGI - VMS Cloud - Kafka  – 172 Troubleshooting of VIGI - VMS Cloud - Kafka\nRD-Cloud&Back-End-Shared – Troubleshooting of VIGI - VMS Cloud\nTroubleshooting of VIGI - VMS Cloud - VMS AI-Manager container memory 0.95  – 183 Troubleshooting of VIGI - VMS Cloud - VMS AI-Manager \ncontainer memory 0.95\nWhen service vms-ai-manager container memory usage exceeds 95%, it is necessary to rotate out the old \npods with high memory usage.", "vms-ai-manager cannot be rotated directly by redeploy.", "It is necessary to first scale up new pods, kick-off \nconnections from the old pods, confirm that the connections have been established to the new pods, and \nthen terminate the old pods.", "1）Scale up hpa min Replicas\nhttps://aps1-rancher-v2.tplinknbu.com/p/c-m-vznhcprp:p-nnsf6/hpa   (can use this rancher address to avoid \nmeeting error frequently when modify hpa)\nselect region and project :\nOn rancher, increase hpa Min Replicas of service vms-ai-manager to cale up new pods as needed, starting \nnew pods to host the connection on the problematic pod.", "RD-Cloud&Back-End-Shared – Troubleshooting of VIGI - VMS Cloud"], "grafana": [], "notes": "", "refs": ["Troubleshooting of VIGI - VMS Cloud-v10-20250926_001218.pdf"]}
{"title": "21 https://pdconfluence.tp-link.com/display/cloudshare/VMS+AI+disconnection+Ops+Guidelines", "keys": ["cat:cpu", "cat:elasticsearch", "cat:memory", "sev:*", "svc:vigi-*"], "priority": "HIGH", "prechecks": [], "actions": ["Troubleshooting of VIGI - VMS Cloud - VMS AI-Manager container memory 0.95  – 192）Kick-off connections from old pods.", "-reference document:  VMS AI disconnection Ops Guidelines21\nAfter new pods successfully started, e xecute the following interface on the container command line of old \npod one by one, disconnect AI Stream.", "curl --location  'http://localhost:10080/v1/internal/vms/ops/smartStream/close'  (see page \n18) --header  'Content-Type: application/json'  --data  '{\"tag\": true, \"number\": 10, \n\"timeInterval\": 2000 }'\n3）Observe grafana dashboard\nhttps://grafana.tplinkcloud.com/d/anrQBsMHr/vms-ai-manager-dashboard?", "orgId=1&from=now-1h&to=now&timezone=browser&var-datasource=f_ml_g-Vk&var-\nnamespace=vms&refresh=5s\nWait for AI stream connections on old pods to drop to zero and for connections connect to new pods.", "Pay attention to the total number of AI stream connections, ensuring it recovers back to original level and \nremains stable.", "4）Delete problem old pods & Recover hpa\nOn rancher, delete problem old pods."], "grafana": [], "notes": "", "refs": ["Troubleshooting of VIGI - VMS Cloud-v10-20250926_001218.pdf"]}
